{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261bbe4a",
   "metadata": {},
   "source": [
    "# ClaudeCrawl example\n",
    "\n",
    "- Claude 를 이용하여 [Firecrawl](https://firecrawl.dev) 와 비슷하게 동작하도록 하는 ClaudeCrawl 구현\n",
    "- DOM 구조를 몰라도 의미적으로 필요한 정보를 가져올 수 있다.\n",
    "\n",
    "## Install Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b910ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U requests langchain langchain_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb72ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_aws.chat_models import ChatBedrockConverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1ee54ef0-c9f7-4ea5-806f-f9ed49715e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "aws_profile_name = None\n",
    "temperature = 0.1\n",
    "max_tokens = 1024 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ae0f3b19-b95b-465a-bfee-a6214a573417",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrockConverse(\n",
    "    model=model_id,\n",
    "    credentials_profile_name=aws_profile_name,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fa2bdacb-0d6d-4f41-8a6b-c263e0846425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article(BaseModel):\n",
    "    \"\"\"\n",
    "    Champion tactic article schema.\n",
    "    Contains details about how to play and win with a specific LOL champion.\n",
    "    \"\"\"\n",
    "\n",
    "    title: str = Field(..., description=\"The title of the article.\")\n",
    "    url: str = Field(..., description=\"The URL link to the article.\")\n",
    "    season: int = Field(..., description=\"The LOL season number for the article.\")\n",
    "    published_at: str = Field(\n",
    "        ..., \n",
    "        description=\"The published date of the article in RFC 3339 format.\"\n",
    "    )\n",
    "\n",
    "class ExtractSchema(BaseModel):\n",
    "    \"\"\"\n",
    "    Schema to extract articles for the tactics from a page.\n",
    "    \"\"\"\n",
    "\n",
    "    articles: List[Article] = Field(\n",
    "        [], \n",
    "        description=\"A list of LOL champion tactic article objects extracted from the page.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3a28d5be-f560-49ce-b947-639358d51bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_output = llm.with_structured_output(ExtractSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "614e894f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://lol.inven.co.kr/dataninfo/champion/manualTool.php?confirm=2&season=14'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f\"https://lol.inven.co.kr/dataninfo/champion/manualTool.php?confirm=2&season=14\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "40e2461c-445b-4684-affe-97203dbe52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are tasked with scraping information from a web page and extracting specific details based on a given output schema. \n",
    "Your task is to carefully read and analyze the content of this web page, and then extract information according to the provided output schema.\n",
    "\n",
    "## Instruction\n",
    "\n",
    "1. To begin, thoroughly read and analyze the entire web page. \\\n",
    "Pay attention to all sections, including headers, paragraphs, lists, tables, and any other relevant elements. \\\n",
    "Take note of the overall structure and organization of the content.\n",
    "2. As you analyze the page, identify information that matches the fields specified in the output schema. \\\n",
    "Be thorough and precise in your extraction.\n",
    "\n",
    "## HTML Analysis\n",
    "- Examine the HTML code and identify elements, classes, or IDs that correspond to each required data field.\n",
    "- Look for patterns or repeated structures that could indicate multiple items (e.g., product listings).\n",
    "- Note any nested structures or relationships between elements that are relevant to the data extraction task.\n",
    "- Discuss any additional considerations based on the specific HTML layout that are crucial for accurate data extraction.\n",
    "- Recommend the specific strategy to use for scraping the content, remeber.\n",
    "\n",
    "## Data Analysis\n",
    "- List out all the links in the page, to make a group by their similarity.\n",
    "- Meaningful data has a tendency to be around a link url, such as `a` tag.\n",
    "- Article links tends to have similar link url, `href` prop, which out numbers the most of the links in the page.\n",
    "\n",
    "## Link Extraction\n",
    "- Do not create any of links, if the content has no link for the schema. \\\n",
    "In that case, just respond with empty string.\n",
    "\n",
    "Begin your scraping process now, and provide the extracted information in the format specified above.\n",
    "Let's think step by step.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "21a2f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_html(soup):\n",
    "    \"\"\"\n",
    "    HTML에서 불필요하게 중첩된 태그들을 정리하는 함수\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): 정리할 HTML 문자열\n",
    "    \n",
    "    Returns:\n",
    "        str: 정리된 HTML 문자열\n",
    "    \"\"\"\n",
    "    def should_remove_tag(tag):\n",
    "        # 태그가 제거되어야 하는 조건\n",
    "        # 1. 내용이 비어있거나 공백뿐인 경우\n",
    "        # 2. 자식 노드가 하나뿐이고 같은 태그인 경우\n",
    "        if not tag.contents:\n",
    "            return True\n",
    "            \n",
    "        if len(tag.contents) == 1:\n",
    "            child = tag.contents[0]\n",
    "            if isinstance(child, type(tag)) and child.name == tag.name:\n",
    "                return True\n",
    "                \n",
    "        text_content = tag.get_text(strip=True)\n",
    "        if not text_content and len(tag.find_all()) == 0:\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "    def clean_tag(tag):\n",
    "        # 재귀적으로 모든 자식 태그들을 정리\n",
    "        for child in tag.find_all(recursive=False):\n",
    "            clean_tag(child)\n",
    "        \n",
    "        if should_remove_tag(tag):\n",
    "            tag.unwrap()\n",
    "    \n",
    "    # 모든 불필요한 공백 제거\n",
    "    for element in soup(text=lambda text: isinstance(text, str)):\n",
    "        if element.strip() == '':\n",
    "            element.extract()\n",
    "    \n",
    "    # 중첩된 태그 정리\n",
    "    clean_tag(soup)\n",
    "    \n",
    "    # 정리된 HTML 반환\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def clean_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # 완전히 제거할 태그들\n",
    "    tags_to_remove = [\n",
    "        'head',\n",
    "        'script',\n",
    "        'style',\n",
    "        'noscript',\n",
    "        'svg',\n",
    "        'meta',\n",
    "        'iframe'\n",
    "        'link',\n",
    "        'video',\n",
    "        'audio',\n",
    "    ]\n",
    "\n",
    "    # 유지할 속성들\n",
    "    keep_attributes = {\n",
    "        'a': ['href', 'title'],\n",
    "        'div': ['id'],\n",
    "        'span': ['id']\n",
    "    }\n",
    "    \n",
    "    # 제거할 태그들 처리\n",
    "    for tag in tags_to_remove:\n",
    "        for element in soup.find_all(tag):\n",
    "            element.decompose()\n",
    "    \n",
    "    # 모든 요소를 순회하면서 불필요한 속성 제거\n",
    "    for element in soup.find_all():\n",
    "        if element.name in keep_attributes:\n",
    "            # 해당 태그에 대해 유지할 속성 목록\n",
    "            allowed_attrs = keep_attributes[element.name]\n",
    "            # 현재 속성들 중 유지할 속성만 필터링\n",
    "            element.attrs = {k: v for k, v in element.attrs.items() if k in allowed_attrs}\n",
    "        else:\n",
    "            # keep_attributes에 정의되지 않은 태그는 모든 속성 제거\n",
    "            element.attrs = {}\n",
    "        \n",
    "        # 공백 문자열 정리\n",
    "        if element.string:\n",
    "            element.string = ' '.join(element.string.split())\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract(html:str):\n",
    "    print(\"extract outputs...\")\n",
    "    prompts = ChatPromptTemplate([\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        (\"human\", \"Here is the web page content.\\n```html\\n{html}\\n```\\n\\nExtract the LOL champion tactic articles at the <table> tag in the page. Each table row is an article.\"),\n",
    "    ]).invoke({ \"html\": html })\n",
    "    return llm_with_output.invoke(prompts)\n",
    "\n",
    "\n",
    "def scrape_url(url):\n",
    "    print(f\"scrape url: {url}...\")\n",
    "    downloaded = requests.get(url, timeout=5).content.decode('utf-8')\n",
    "    cleaned_soup = clean_html(downloaded)\n",
    "    flat_html = flatten_html(cleaned_soup)\n",
    "    return extract(flat_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7c738eb5-9400-4ff1-bf72-6d6db8f2b258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrape url: https://lol.inven.co.kr/dataninfo/champion/manualTool.php?confirm=2&season=14...\n",
      "extract outputs...\n",
      "CPU times: user 239 ms, sys: 11.6 ms, total: 251 ms\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response = scrape_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "608c6ab4-ed37-492e-9792-3c38e2401cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Article(title='[GM]AP 샤코 서폿 설명 길게 안함', url='https://lol.inven.co.kr/dataninfo/champion/manualToolView.php?idx=146545', season=14, published_at='2023-09-22T00:00:00Z'),\n",
       " Article(title='★(마스터) 시즌 완벽 적응 개사기', url='https://lol.inven.co.kr/dataninfo/champion/manualToolView.php?idx=148044', season=14, published_at='2023-07-26T00:00:00Z'),\n",
       " Article(title='[GM1]프로 1군원딜들 피셜 근본 원딜', url='https://lol.inven.co.kr/dataninfo/champion/manualToolView.php?idx=148020', season=14, published_at='2023-03-16T00:00:00Z'),\n",
       " Article(title='M)시즌 5부터 딩거 한 유저의 공략', url='https://lol.inven.co.kr/dataninfo/champion/manualToolView.php?idx=148020', season=14, published_at='2023-02-05T00:00:00Z')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(response.articles))\n",
    "response.articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fb160-3112-4074-80d4-f454ca3ef859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
